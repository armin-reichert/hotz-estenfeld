\section{Grammars and derivations}

In the introduction of this book we already learned about formal languages. The
wish to describe these in general infinite sets of words by a finite generating
system leads to the notion of a {\bf grammar}.

\begin{definition}
$G = (N, T, P, S)$ is a {\bf Chomsky grammar}, if
\begin{enumerate}
  \item $N$ is a finite non-empty set of {\bf nonterminal symbols}
  \item $T$ is a finite non-empty set of {\bf terminal symbols} with
  $N \cap T = \emptyset$
  \item $P \subset N^+ \times (N \cup T)^*$ is a finite set of {\bf productions}
  \item $S \in N$ is the {\bf axiom} or {\bf start symbol} 
\end{enumerate}
\end{definition}

Notation: For $p = (u, v) \in P$ we also write $u \production{p} v$ and
$Q(p) = u,\ Z(p) = v$ denote the {\em source} and {\em target} of a production.

Examples:
\begin{enumerate}
  \item $G_1 = (N, T, P, S)$ with $N = \setof{S}, T = \setof{x, x'},$ \\ 
  $P = \setof{S \to SS,\ S \to xSx',\ S \to \epsilon}$
  \item $G_2 = (N, T, P, S)$ with $N = \setof{S, X},\ T = \setof{x, x'}$ \\
  $P = \setof{S \to xSX,\ S \to xX, X \to x'S,\ X \to x'}$
\end{enumerate}

To use a grammar for generating the words of a language, starting with the axiom
intermediate words are generated by application of the productions until
the produced word contains only terminal symbols. This leads to the
notion of a {\em derivation} that will now be formally defined.

\begin{definition}
Let $G = (N, T, P, S)$ be a grammar and $w, w' \in (N \cup T)^*$.

$w'$ is {\bf directly derivable} from $w$ in $G$ ($w \dderives{G} w'$) if there
are decompositions 
\[ w = w_1 \cdot u \cdot w_2\text{ and }w' = w_1 \cdot v \cdot w_1 \]
and there is a production $(u, v) \in P$.

$w'$ is {\bf derivable} from $w$ ($w \derives{G} w'$) if there exists
a sequence of words 
\[ w = w_0, \ldots, w_n = w',\quad n \in \mathbb{N},\ w_i \in (N \cup T)^* \]
such that $w_i \dderives{G} w_{i+1},\ 0 \leq i \leq n$.
\end{definition}

Such a sequence is called a {\bf derivation} of length $n$. $Q$ and $Z$ can be
extended in a natural way to derivations.

A derivation is called {\bf canonic} or {\bf leftmost} if in each step $w
\dderives{G} w'$ the following holds:

If $w = w_1 \cdot u \cdot w_2$ and $w' = w_1 \cdot v \cdot w_2$ are the
decompositions and $(u, v) \in P$ is the applied production, then $w_1 \in T^*$.
This means that always the leftmost nonterminal is replaced.

If the grammar is known, we omit $G$ from the relation
symbols $\dderives{G}$ and $\derives{G}$. 

Let's consider some properties of the
relation $\derives{G}$:

\begin{lemma}
Let $G$ be a grammar. It holds:
\begin{enumerate}
  \item $(u, v) \in P \ \Rightarrow\ u \derives{G} v$
  \item $w \derives{G} w$ (reflexivity)
  \item $w \derives{G} w' \wedge w' \derives{G} w'' \ \Rightarrow\ w \derives{G}
  w''$ (transitivity)
  \item $w_1 \derives{G} w'_1 \wedge w_2 \derives{G} w'_2\ \Rightarrow\ w_1
  \cdot w_2 \derives{G} w'_1 \cdot w'_2$ (compatibility with monoid operation)
\end{enumerate}
Here, $w, w', w'', w_1, w_2, w'_1, w'_2 \in (N \cup T)^*$.
\end{lemma}

Proof:
\begin{enumerate}
  \item follows from the definition of $\derives{G}$.
  \item clear with $n = 0$ in the definition of $\derives{G}$.
  \item There exist sequences $w = w_0, \ldots, w_n = w',\ w' = w'_0, \ldots,
  w'_m = w''$ with $w_i \derives{G} w_{i+1}$ and $w'_j \derives{G} w'_{j+1}$.
  Because $w_n = w' = w'_0$, the composed sequence $w = w_0, \cdots, w_n, w'_1,
  \cdots, w'_n = w''$ is a derivation from $w$ to $w''$.
  \item Exercise for the reader
\end{enumerate}

Notation: An intermediate word that is generated by a derivation starting with
the axiom is called a {\bf sentence form} of $G$. We define:

\begin{definition}
\[ SF(G) := \setof{w \in (N \cup T)^* \mid S \derives{G} w} \] is the set of
{\bf sentence forms} of $G$.
\end{definition}

Now we are able to define the formal language generated by a grammar.

\begin{definition}
Let $G = (N, T, P, S)$ be a grammar. \[ L(G) := \setof{w \in T^* \mid S
\derives{G} w} \] is the {\bf language generated by} $G$.
\end{definition}

Note: $L(G) = SF(G) \cap T^*$.

Examples:
\begin{enumerate}
  \item One can see that for the grammar $G_1$ from example 1 it holds: $L(G_1)$
  is the Dyck language over the alphabet $\setof{x, x'}$.
  \item Let $G = (N, T, P, S)$ with $N = \setof{S}, T = \setof{a, b}, P =
  \setof{S \to aSb, S \to \epsilon}$. Then $L(G) = \setof{a^n b^n \mid n \in
  \mathbb{N}}$. The simple proof is left to the reader.
\end{enumerate}

Grammars are compared with respect to the languages they generate. We define:

\begin{definition}
$G$ is {\bf weakly equivalent} to $G' \iff L(G) = L(G')$.
\end{definition}

Remark: The reader should convince himself that the grammars $G_1$ and $G_2$
from the first example generate the same language.

Of course you can define infinitely many different grammars for each language.

We can define now different classes of grammars (and languages generated by
them) depending on certain restrictions of their production systems. In the next
chapter we will meet the so called {\em right-linear} grammars and their
languages.

Of special importance, also from a practical point of view, are the so
called {\em context-free} grammars.

\begin{definition}
A grammar $G = (N, T, P, S)$ is called {\bf context-free} if $P \subset N \times
(N \cup T)^*$.
\end{definition}

The term {\em context-free} describes the fact that in a sentence-form a
nonterminal symbol may be replaced by the right-hand side of a production
without having to take the ''context'' of that symbol, i.e.\ the symbols to the
left or right, into account.

In chapter 4 we will treat context-free grammars in depth.





























